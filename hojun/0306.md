# Foundation Models in Robotics

## Foundation Models in Background

### A. Terminology and Mathematical Preliminaries
- Tokenization : 텍스트 쪼개기
- Generative Models **vs** Discriminative Models
  - 생성형 : 데이터의 확률 분포를 학습하고, 이로부터 새로운 샘플을 생성 - GPT, DALL-E
  - 판별형 : 입력 데이터에 대해 분류, 예측 - CNN, ML, ...
- Transformer Architecture : 순차 데이터도 모든 위치 쌍에 대해 병렬로 attention 연산
  - self-attention : 입력 시퀀스 내 토큰들끼리의 관련성 학습
  - multi-head : 어텐션 연산을 병렬로 여러번 수행
  - 위치 인코딩
- Contrastive Learning : 양의 샘플 쌍은 가깝게, 음의 샘플 쌍은 멀어지게
  - CLIP, BLIP : 이미지-텍스트 쌍 데이터 > 로봇 응용
- Diffusion Models
  - 정방향(Forward) 과정 : 데이터에 노이즈를 입힘
  - 역방향(Reverse) 과정 : 노이즈를 제거하고 복원

### B. Large Language Model (LLM) Examples and Historical Context
- 대표 모델
  - BERT, Masked Language Model : 문장 중 일부 단어(토큰)을 마스킹(mask)하고, 이를 예측하도록 학습
  - GPT, Autoregressive Language Model : 앞 단어들이 주어졌을 때 다음 단어를 예측하도록 학습
  - 초거대 모델
    - Fine-tuning & Prompting
    - RLHF(Reinforcement Learning with Human Feedback)
  - 추론 모델 (Reasoning, Test-Time Computing)
    - Chain of Thought
  - 경량화
    - 가지치기, Prunning
    - 양자화, Quantization
    - 지식 증류, Knowledge Distillation
- How LLM can be Foundation Model?
  - Generalization
  - Emergent
  - Plug-and-Play
- 로보틱스 분야와의 접점
  - 로봇에게 고수준 명령(“물건 집어서 옮겨 둬” 등)을 텍스트로 전달하고, 로봇이 이를 적절히 해석·실행하도록 하는 청사진을 제시합니다.
  - 로봇이 언어 명령을 이해하려면, LLM이 가진 개념적 지식, 추론 능력이 중요하고, 이를 로봇의 감각 정보(이미지·센서)와 연동해야 한다는 점에서, LLM은 로봇용 파운데이션 모델과 자연스럽게 연결되는 토대가 되고 있습니다.

### C. Vision Transformers
기존에는 합성곱 신경망(CNN)을 주로 사용하던 이미지 기반 문제들을, Transformer 기반으로 해결하려는 시도가 ViT 계열에서 시작
- 개념
  1. 이미지를 '토큰(패치)' 시퀀스로 처리
    - NLP에서 단어를 ‘토큰(token)’으로 간주하듯, ViT는 이미지를 일정 크기의 패치(Patch)로 분할하고, 각 패치를 ‘토큰’처럼 취급합니다.
    - 이처럼 생성된 패치를 1D로 펼친 뒤, 고정 차원의 벡터로 임베딩(embedding)하여 Transformer에 입력합니다.
  2. 위치 인코딩
  3. 멀티 헤드 셀프 어텐션
  4. 확장성

### D. Multimodal Vision-Language Models (VLMs)
- 개념
  - 비전(Vision)과 언어(Language)를 동시에 다루는 멀티모달 학습 기법
  - 단순히 텍스트만 이해하거나 이미지만 분석해서는 얻을 수 없는 풍부한 ‘상호연결 의미’를 학습하는 것이 목표
  - 이미지에는 언어가 지니는 추상적 의미나 순차적 맥락이 부족할 수 있고, 언어에는 구체적인 시각적 정보가 결핍될 수 있음, 상호보완 목적적
- 예시
  - Contrastive Language-Image Pre-training (CLIP)
    - 대규모 웹 이미지와 해당 이미지에 대한 캡션(설명문) 쌍을 모아, 대조학습(contrastive learning)을 수행합니다.
  - 멀티모달 트랜스포머 기반 학습
    - 시각 모델과 언어 모델을 병렬로 두고, 두 임베딩을 합치는 병합(Merge) 구조나, 교차어텐션(Cross-attention)으로 상호작용하는 구조 등 다양한 아키텍처들이 존재
    - 공통적으로 이미지·텍스트를 같은 차원의 “토큰 시퀀스”로 보고, 트랜스포머로 처리하는 방식이 주가 되며, 이 과정에서 언어-시각 간 의미적 일치를 극대화하는 방향으로 학습
- 로봇에서의 응용 예시
  - 시각적 의미 추출 + 언어 지시 기반 행동
  - 정확한 3D Task에 활용

### E. Embodied Multimodal Language Models
- 개념
  - An embodied agent is an AI system that interacts with a virtual or physical world
  -  Embodied language models are foundation models that incorporate real-world sensor and actuation modalities into pretrained large language models. 
  - 로봇이 언어로 주어진 목표나 설명을 이해하고 실제 동작까지 이어가게 하는 것을 목표로 합니다.
- PaLE-E
  - 구조 : 대규모 언어 모델과 ViT 등을 결합해, 언어·이미지·센서 정보를 동시에 입력받을 수 있도록 설계
  - 학습 : 대규모 LLM을 바탕으로, 추가적인 로봇 데이터(시뮬레이션 혹은 실제 로봇 환경)를 함께 학습
  - 출력 : 다음에 취해야 할 조치를 자연어(혹은 프로그래밍 형태)로 기술하거나, 로봇 제어 신호를 직간접적으로 표현

### F. Visual Generative Models
- Diffusion 기반 생성 모델
  - 순방향 프로세스(forward process) : 이미지에 점진적으로 노이즈를 추가해 백색 잡음(white noise)에 가깝게 만들고,
  - 역방향 프로세스(reverse process) : 모델이 이 노이즈를 단계별로 되돌려가는 식으로 최종 이미지를 복원·합성하는 방식
  - 노이즈 -> 이미지 역과정 학습
